---
title: "Multi-Class_Prediction_of_Obesity_Risk"
author: "Ronald Nguyen 222200308"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
---
# Brief description of all the data in the dataset

id: A unique identifier for each individual in the dataset.

Gender: The individual's gender, indicating whether they are male or female.

Age: The age of the individual, representing their age in years.

Height: The height of the individual, typically measured in meters.

Weight: The weight of the individual, typically measured in kilograms.

family_history_with_overweight: Indicates whether there is a family history of overweight for the individual (yes/no).

FAVC: Stands for "Frequency of consuming high caloric food," representing how often the individual consumes high-calorie foods (yes/no).

FCVC: Stands for "Frequency of consuming vegetables," representing how often the individual consumes vegetables.

NCP: Stands for "Number of main meals," indicating the number of main meals the individual consumes daily.

CAEC: Stands for "Consumption of food between meals," representing the frequency of consuming food between meals.

SMOKE: Indicates whether the individual smokes or not (yes/no).

CH2O: Represents the amount of water consumption for the individual.

SCC: Stands for "Calories consumption monitoring," indicating whether the individual monitors their calorie consumption (yes/no).

FAF: Stands for "Physical activity frequency," representing the frequency of the individual's physical activities.

TUE: Stands for "Time using technology devices," indicating the amount of time the individual spends using technology devices.

CALC: Stands for "Consumption of alcohol," representing the frequency of alcohol consumption.

MTRANS: Stands for "Mode of transportation," indicating the mode of transportation the individual uses.

NObeyesdad: The target variable, representing the obesity risk category of the individual. It has multiple classes such as 'Overweight_Level_II', 'Normal_Weight', 'Insufficient_Weight', 'Obesity_Type_III', 'Obesity_Type_II', 'Overweight_Level_I', and 'Obesity_Type_I'.

# Load and install libraries
```{r}
# Install required packages if they are not already installed
if (!require("caret")) install.packages("caret", dependencies = TRUE)
if (!require("class")) install.packages("class", dependencies = TRUE)
if (!require("dplyr")) install.packages("dplyr", dependencies = TRUE)
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE)
if (!require("tidyverse")) install.packages("tidyverse", dependencies = TRUE)
if (!require("corrplot")) install.packages("corrplot", dependencies = TRUE)
if (!require("randomForest")) install.packages("randomForest", dependencies = TRUE)
if (!require("nnet")) install.packages("nnet", dependencies = TRUE)
if (!require("rpart")) install.packages("rpart", dependencies = TRUE)

# Load libraries
library(rpart)
library(nnet)
library(caret) 
library(class) 
library(dplyr) 
library(ggplot2)
library(tidyverse)
library(corrplot) 
library(randomForest)
```

# Set project parameters

```{r}
set.seed(42)  # set random seed for reproducible outputs

# Plotting size function 
set_plot_dimensions <- function(width_choice, height_choice) {
    options(repr.plot.width=width_choice, repr.plot.height=height_choice)}
```

# Load and Exploring Data

```{r}
df_train <- read.csv("train.csv", row.names=1) # train data (to be split)
df_test <- read.csv("test.csv", row.names=1)  # unseen data 
sub <- read.csv("sample_submission.csv")  # submission file
head(df_train)

```

# Exploratory Data Analysis
This helps to understand what type of models are used.

```{r}

# plot 
ggplot(df_train, aes(x = NObeyesdad, fill = factor(NObeyesdad))) +
  geom_bar() +
  labs(title = "Count of Target (NObeyesdad)",
       x = "NObeyesdad",
       y = "Count") +
  theme(
    plot.title = element_text(size = 14, face = "bold"),  
    axis.title.x = element_text(size = 12),              
    axis.title.y = element_text(size = 12),              
    axis.text.x = element_text(size = 5),               
    axis.text.y = element_text(size = 10)                 
  )
```

# Correlation

Correlation between all numeric variables. If the correlation factor is positive, then it is a positive correlation and if it is negative, it is a negative correlation. If the factor is near 0, then there is almost no correlation between these variables.
```{r}
set_plot_dimensions(12,12)
corrplot(df_train %>%  select_if(is.numeric) %>% cor() ,
         tl.cex = 1.1,
         method = "color", order = 'hclust', addCoef.col = 'black',type = "upper",tl.pos = 'd',
         )
```

# Feature analysis
Basic investigation to look for duplicates and null values. 

```{r}
# duplicates across train/ test
sum( 
    duplicated(rbind(df_train[, -17],df_test))
    )

# null values by column  
colSums(is.na(df_train))
colSums(is.na(df_test))
```

# Look at other variables which may need engineering for example MTRANS

```{r}

set_plot_dimensions(20,7)

ggplot(df_train, aes(x = MTRANS, fill = factor(MTRANS))) +
  geom_bar() +
  labs(title = "Count of Target (MTRANS)",
       x = "NObeyesdad",
       y = "Count")
```

# Convert all character type variables to factors

```{r}

df_train <- df_train %>% mutate_if(is.character,as.factor)
df_test <- df_test %>% mutate_if(is.character,as.factor)

combined_df <- rbind(df_train[,-17], df_test)

# Perform one-hot encoding
# one all columns except the target 
combined_df_encoded <- model.matrix(~. - 1, data = combined_df)

# Split the datasets back
train_encoded <- data.frame( 
    combined_df_encoded[1:nrow(df_train), ])

# Combine
df_test <- data.frame(
    combined_df_encoded[(nrow(df_train) + 1):nrow(combined_df_encoded), ]
    )
df_train <- cbind(train_encoded ,NObeyesdad= df_train$NObeyesdad)
df_train <- df_train %>% select(-CALCAlways)
head(df_train)
```
# Adding BMI column adjusted by age and gender
```{r}
calculate_BMI <- function(weight, height, age, gender_male) {
  bmi <- weight / (height ^ 2)
  
  # Adjust BMI for age and gender
  if (gender_male == 1) {
    if (age < 18) {
      bmi <- bmi * 1.1
    } else if (age >= 18 & age <= 24) {
      bmi <- bmi * 1.05
    }
  } else {
    if (age < 18) {
      bmi <- bmi * 1.15
    } else if (age >= 18 & age <= 24) {
      bmi <- bmi * 1.08
    }
  }
  
  return(bmi)
}

df_train <- df_train %>%
  rowwise() %>%
  mutate(BMI = calculate_BMI(Weight, Height, Age, GenderMale))

df_test <- df_test %>%
  rowwise() %>%
  mutate(BMI = calculate_BMI(Weight, Height, Age, GenderMale))

str(df_train)
```

# Correlation Matrix with all variables

```{r}
# calculate the correlations in the training set
dataset_corr <- df_train %>% select_if(is.numeric) %>% cor(use = "complete.obs")

# function to plot the heatmap 
plot_heatmap_correlation <- function(dataset) {
  set_plot_dimensions(10, 10)  
  
corrplot(dataset, method = "color", order = 'hclust', addCoef.col = 'black', 
           tl.cex = 0.4, number.cex = 0.3, type = "upper", tl.pos = 'lt', col = colorRampPalette(c("red", "white", "blue"))(200))


}

# plotting the heatmap
dataset_corr <- dataset_corr[, colSums(is.na(dataset_corr)) != nrow(dataset_corr)]
dataset_corr <- dataset_corr[rowSums(is.na(dataset_corr)) != ncol(dataset_corr), ]
plot_heatmap_correlation(dataset_corr)

```

# Split/Partition data
Seperate our "Train" data into training and test data (not to be confused with df_test) which is our unseen data.
First the Dataset is being partitioned 70% is being used by the train_set and the rest is for the test_set. 
The Datasets are also splitted by X und y (X are all feature variables and y is the target variable (NObeyesdad))
```{r}

# Get train index
train_idx <- createDataPartition(y = df_train$NObeyesdad, p = 0.7, list = FALSE)

# Partition data into training and test sets
df_train_set <- df_train[train_idx, ]
df_test_set <- df_train[-train_idx, ]

# Split the data into X (features) and y (target)
X_train <- df_train_set %>% select(-NObeyesdad)
X_test <- df_test_set %>% select(-NObeyesdad)

y_train <- as.factor(df_train_set$NObeyesdad)
y_test <- as.factor(df_test_set$NObeyesdad)
```

# Scale
The Dataset is now scaled by centralizing (mean = 0) and scaled (standard deviation = 1) this is required for logistic regression. It can reduce training time and improve model accuracy
```{r}
# create the scaler function on the train data 
scaler <- preProcess(x= X_train, method = c("center", "scale"))

# normalize the training date 
X_train_s <- predict(scaler,X_train)  
X_test_s <- predict(scaler,X_test) 
```

# Random Forest
Random Forest is a Bagging ensemble (sample bootstrapping) with random feature select ioin.
The error of the Random Forest model decreases as the number of trees increases. 
```{r}
rf_model <- randomForest(y_train~. , data = cbind(X_train,y_train),
                         ntree = 1000, # total number of decision trees in ensemble --> larger is more accurate but slower ro run
                         mtry= round(sqrt(ncol(df_train))), # number of features (potential for hyperparameter tuning)
                         nodesize = 20 # minimum number of terminal leaf nodes --> larger the number the smaller the tree 
                        )



par(mar = c(5, 4, 4, 15) + 0.1)
plot(rf_model, main = "Random Forest: Error per number of trees")
# Place the legend outside the plot
legend("topright", inset = c(-0.7, 0), legend = colnames(rf_model$err.rate), 
       col = 1:ncol(rf_model$err.rate), lty = 1:ncol(rf_model$err.rate), xpd = TRUE)
```

## Prediction

```{r}
y_probs <- predict(rf_model, newdata = X_test, type = "prob")
y_preds <- predict(rf_model, newdata = X_test, type = "class")
head(y_preds)
```
## Evaluation

```{r}
cm<- confusionMatrix(y_preds, y_test)
cm
# calculate the feature importances
importance_data <- as.data.frame(rf_model$importance)
importance_data$Feature <- rownames(importance_data)
rownames(importance_data) <- NULL

# sort the features by importanc
importance_data <- importance_data %>%
  arrange(desc(MeanDecreaseGini))

# change the order of the levels of the features
importance_data$Feature <- factor(importance_data$Feature, levels = importance_data$Feature)

# table of the importance of the data
print(importance_data)

ggplot(importance_data, aes(x = reorder(Feature, -MeanDecreaseGini), y = MeanDecreaseGini, fill = Feature)) +  
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 4), # Schriftgröße
        axis.text.y = element_text(size = 10), 
        plot.title = element_text(size = 14, face = "bold"),
        legend.title = element_text(size = 12), # Schriftgröße der Legendentitel
        legend.text = element_text(size = 7)) + # Schriftgröße der Legendentexte
  labs(x = "Feature", y = "Mean Decrease Gini", title = "Feature Importances from Random Forest Model")

```

# Decision Tree
Plotting the decision tree
```{r}
# Fit the decision tree model
tree_model <- rpart(NObeyesdad ~ ., data = df_train_set, method = "class")

# Plot the decision tree
plot(tree_model, uniform = TRUE, main = "Decision Tree for Obesity Prediction", cex.main = 0.5, cex.lab = 0.01)

# Add text labels to the decision tree
text(tree_model, use.n = TRUE, all = TRUE, cex = 0.5)  # Adjust 'cex' to change text size

```

## Prediction and evaluation of the decision tree
```{r}
tree_pred <- predict(tree_model, df_test_set, type = "class")
tree_cm <- confusionMatrix(tree_pred, df_test_set$NObeyesdad)
print(tree_cm)
```


# logistic regression model
```{r}
# Fit the logistic regression model
glm_model <- multinom(NObeyesdad ~ ., data = df_train_set)
```

## Predict and Evaluate

```{r}
# Predict on the test set
glm_pred <- predict(glm_model, newdata = df_test_set)

# Evaluate the model
glm_cm <- confusionMatrix(glm_pred, df_test_set$NObeyesdad)
print(glm_cm)
```

# Submit random forest model
This submission got a public score of 0.88908.

```{r}
sub_preds <- predict(rf_model,as.matrix(df_test))
head(sub_preds)
# create a mapping vector
mapping_vector <- setNames(levels(y_train), 1:7)

# Converting numbers back to text
sub$NObeyesdad<- mapping_vector[sub_preds]

head(sub)
write.csv(sub, "submission_rf.csv", row.names = FALSE)
```
# Improved random tree forest through feature importance and correlations

```{r}
correlation_matrix <- cor(df_train %>% select_if(is.numeric), use = "complete.obs")

importance_data <- as.data.frame(rf_model$importance)
importance_data$Feature <- rownames(importance_data)
rownames(importance_data) <- NULL

importance_data <- importance_data %>%
  arrange(desc(MeanDecreaseGini))

# delete high correlated and less important features
high_correlation_threshold <- 0.9
low_importance_threshold <- quantile(importance_data$MeanDecreaseGini, 0.2)

highly_correlated_features <- findCorrelation(correlation_matrix, cutoff = high_correlation_threshold)
low_importance_features <- importance_data$Feature[importance_data$MeanDecreaseGini < low_importance_threshold]

# delete specific features
features_to_remove <- union(names(highly_correlated_features), low_importance_features)
df_train_reduced <- df_train_set %>% select(-one_of(features_to_remove))
df_test_reduced <- df_test_set %>% select(-one_of(features_to_remove))

X_train_reduced <- df_train_reduced %>% select(-NObeyesdad)
X_test_reduced <- df_test_reduced %>% select(-NObeyesdad)

y_train_reduced <- as.factor(df_train_reduced$NObeyesdad)
y_test_reduced <- as.factor(df_test_reduced$NObeyesdad)

# training of the reduced random forest model
rf_model_reduced <- randomForest(y_train_reduced ~ ., data = cbind(X_train_reduced, y_train_reduced), 
                                 ntree = 1000, mtry = round(sqrt(ncol(df_train_reduced))))

# predictions on the test set
y_probs_reduced <- predict(rf_model_reduced, newdata = X_test_reduced, type = "prob")
y_preds_reduced <- predict(rf_model_reduced, newdata = X_test_reduced, type = "class")

# evaluation
cm_reduced <- confusionMatrix(y_preds_reduced, y_test_reduced)
print(cm_reduced)

# convert the confusion matrix into a data frame
cm_df <- as.data.frame(cm_reduced$table)


# Plot the error
plot(rf_model_reduced, main = "Random Forest: Error per number of trees")
```
## Evaluation
```{r}
# calculate the accuracy
accuracy <- sum(diag(cm_reduced$table)) / sum(cm_reduced$table)
accuracy_label <- paste0("Accuracy: ", round(accuracy * 100, 2), "%")
# create the heatmpa
ggplot(data = cm_df, aes(x = Prediction, y = Reference)) +
  geom_tile(aes(fill = Freq), color = "white") +
  scale_fill_gradient(low = "white", high = "blue") +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "Confusion Matrix", x = "Predicted Label", y = "True Label") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  annotate("text", x = 10, y = 1, label = paste0("Accuracy: ", round(accuracy, 4)), vjust = 1.5, hjust = 1.1, size = 4, color = "black", Position = "bottom")
```

# Submit improved random forest model
This submission got a public score of 0.89884.

```{r}
sub_preds <- predict(rf_model_reduced,as.matrix(df_test))
head(sub_preds)
# create a mapping vector
mapping_vector <- setNames(levels(y_test), 1:7)

# Converting numbers back to text
sub$NObeyesdad<- mapping_vector[sub_preds]

head(sub)
write.csv(sub, "submission_improved_rf.csv", row.names = FALSE)
```

# comparison between all models

```{r}
results <- list()
results$Reduced_Random_Forest <- cm_reduced$overall['Accuracy']
results$Logistic_Regression <- glm_cm$overall['Accuracy']
results$Decision_Tree <- tree_cm$overall['Accuracy']
results$Random_Forest <- cm$overall['Accuracy']

results_df <- data.frame(Model = names(results),
                         Accuracy = unlist(results))


ggplot(results_df, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Comparision: Accuracy",
       x = "Model",
       y = "Accuracy") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


